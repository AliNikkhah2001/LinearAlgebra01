





get_ipython().run_line_magic("matplotlib", " inline")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

dataset = pd.read_csv('arac.csv', sep=';')
BaseYear = 1966

x = np.matrix(dataset.Year[0:] - BaseYear).T
cars = np.matrix(dataset.Car[0:]).T
buses = np.matrix(dataset.Bus[0:]).T

# Define degrees of polynomials
degrees = [3, 4, 5]

for K in degrees:
    # Create Vandermonde Matrix
    A = np.hstack([np.power(x, i) for i in range(K + 1)])

    # Find the W matrix for cars
    w, _, _, _ = np.linalg.lstsq(A, cars, rcond=None)

    # Prediction for cars
    TargetYears = np.matrix([year - BaseYear for year in [2016, 2017, 2018, 2019, 2020]]).T

    A2 = np.hstack([np.power(TargetYears, i) for i in range(K + 1)])
    f2 = A2 * w

    # Plot the results for cars
    plt.figure(figsize=(8, 6))
    plt.plot(dataset.Year[0:], cars, 'o-', label='Actual Data')
    plt.plot([2016, 2017, 2018, 2019, 2020], f2, 'ro-', label=f'Degree {K} Fit')
    plt.title(f'Prediction of Cars (Degree {K})')
    plt.xlabel('Year')
    plt.ylabel('Number of Cars')
    plt.legend()
    plt.grid(True)
    plt.show()

    print(f"Predicted Numbers for cars (Degree {K}):")
    print(f2)

    # Find the W matrix for buses
    w, _, _, _ = np.linalg.lstsq(A, buses, rcond=None)

    # Prediction for buses
    f2 = A2 * w

    # Plot the results for buses
    plt.figure(figsize=(8, 6))
    plt.plot(dataset.Year[0:], buses, 'o-', label='Actual Data')
    plt.plot([2016, 2017, 2018, 2019, 2020], f2, 'ro-', label=f'Degree {K} Fit')
    plt.title(f'Prediction of Buses (Degree {K})')
    plt.xlabel('Year')
    plt.ylabel('Number of Buses')
    plt.legend()
    plt.grid(True)
    plt.show()

    print(f"Predicted Numbers for buses (Degree {K}):")
    print(f2)






import numpy as np

# Create sample points
n = 10
x = np.linspace(0, 5, n)
epsilon = np.random.normal(0, 1, n)
y = 2 * x + 3 + epsilon



# Calculate least squares estimates for a and b
A = np.vstack([x, np.ones(len(x))]).T
a, b = np.linalg.lstsq(A, y, rcond=None)[0]



import matplotlib.pyplot as plt

# Plot sample points and estimated line
plt.scatter(x, y, label='Sample Points')
plt.plot(x, a*x + b, 'r', label='Estimated Line')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Linear Least Squares Fit')
plt.legend()
plt.show()



# Define function for error calculation
def calculate_error(a, b, x, y):
    error = np.sum((a*x + b - y)**2)
    return error

# Calculate errors for different sample sizes
n_values = np.arange(10, 51)
errors = []
for n in n_values:
    x = np.linspace(0, 5, n)
    epsilon = np.random.normal(0, 1, n)
    y = 2 * x + 3 + epsilon
    A = np.vstack([x, np.ones(len(x))]).T
    a, b = np.linalg.lstsq(A, y, rcond=None)[0]
    errors.append(calculate_error(a, b, x, y))

# Plot errors
plt.plot(n_values, errors)
plt.xlabel('Number of Samples')
plt.ylabel('Error')
plt.title('Error vs. Number of Samples')
plt.show()



# Create sample points with quadratic term
n = 20
x = np.linspace(0, 5, n)
epsilon = np.random.normal(0, 1, n)
y = x**2 + 2 * x + 3 + epsilon



# Calculate least squares estimates for quadratic function
A = np.vstack([x**2, x, np.ones(len(x))]).T
coefficients = np.linalg.lstsq(A, y, rcond=None)[0]



# Plot sample points and estimated quadratic function
plt.scatter(x, y, label='Sample Points')
plt.plot(x, coefficients[0]*x**2 + coefficients[1]*x + coefficients[2], 'r', label='Estimated Quadratic')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Quadratic Least Squares Fit')
plt.legend()
plt.show()









import numpy as np

P = np.array([
    [0, 0.2, 0.3, 0.4, 0.1],
    [0.3, 0, 0.2, 0.1, 0.4],
    [0.2, 0.3, 0, 0.2, 0.3],
    [0.1, 0.2, 0.4, 0, 0.1],
    [0.4, 0.3, 0.1, 0.3, 0.1]
])

X_0 = np.array([0, 0, 0, 1, 0])  # Starting from page 4



def compute_navigation_probabilities(P, X_0, t):
    X_t = np.linalg.matrix_power(P, t) @ X_0
    return X_t

t = 10
X_t = compute_navigation_probabilities(P, X_0, t)
print("Probabilities after", t, "transitions:", X_t)









eigenvalues, eigenvectors = np.linalg.eig(P)
print("Eigenvalues:", eigenvalues)
print("Eigenvectors:", eigenvectors)






for i in range(len(eigenvalues)):
    print(np.isclose(P @ eigenvectors[:, i], eigenvalues[i] * eigenvectors[:, i]))









# Find eigenvector corresponding to eigenvalue 1
steady_state_index = np.where(np.isclose(eigenvalues, 1))[0][0]
steady_state_vector = eigenvectors[:, steady_state_index]

# Normalize eigenvector to represent the steady-state distribution of page ranks
steady_state_vector /= np.sum(steady_state_vector)

print("Probabilities being at each of the pages:\n" + str(steady_state_vector))



# Calculate probabilities from the steady state vector
X_inf = intsteady_state_vector

# Normalize to ensure it satisfies the probability vector criteria
X_inf /= np.sum(X_inf)

print("Probabilities being at each of the pages:\n" + str(X_inf))






# Example transition matrix for a small web model, including a dangling node
# Assume 4 pages, with the 4th page being a dangling node
P = np.array([
    [0, 0, 0, 0, 0],     # Probabilities from each page to Page 1
    [1, 0, 0.5, 0, 0],   # Probabilities from each page to Page 2
    [0, 0, 0, 0.5, 0],   # Probabilities from each page to Page 3
    [0, 1, 0.5, 0, 0],   # Probabilities from each page to Page 4
    [0, 0, 0, 0.5, 0]    # Page 5 is a dangling node
])

# Identify dangling nodes
dangling_nodes = np.where(~P.any(axis=0))[0]

# Adjust transition matrix P
n = P.shape[0]
P_damped = P.copy()
P_damped[:, dangling_nodes] = 1 / n

# Apply damping factor
d = 0.85
P_damped = d * P_damped + (1 - d) / n

print("Damped Transition Matrix:")
print(P_damped)






import scipy.sparse.linalg as spla

# Apply damping factor to the transition matrix
damping_factor = 0.85
n = P.shape[0]  # Assuming P is already defined
E = np.ones_like(P) / n
P_damped = damping_factor * P + (1 - damping_factor) * E
print(P_damped)





# Sparse matrix representation and power iteration algorithm
import scipy.sparse as sp

# Convert P to sparse matrix
P_sparse = sp.csr_matrix(P)
print(P_sparse)
# Convert P_damped to sparse matrix
P_damped_sparse = sp.csr_matrix(P_damped)
print(P_damped_sparse)






# Compute the principal eigenvector using sparse matrix techniques and power iteration
def power_iteration_sparse(matrix, max_iterations=1000, tol=1e-6):
    n = matrix.shape[0]
    x = np.ones(n) / n  # Initial guess for the eigenvector
    for _ in range(max_iterations):
        x_new = matrix.dot(x)
        x_new /= np.linalg.norm(x_new, ord=1)  # Normalize
        if np.linalg.norm(x_new - x, ord=np.inf) < tol:
            break
        x = x_new
    return x

# Compute the principal eigenvector using power iteration with sparse matrices
principal_eigenvector_sparse = power_iteration_sparse(P_damped_sparse.T)

# Normalize the principal eigenvector
principal_eigenvector_sparse /= np.sum(principal_eigenvector_sparse)
print(principal_eigenvector_sparse)





def pageRank(X):
    # Normalize matrix X
    X_norm = X / np.sum(X, axis=0)
    
    # Apply damping factor
    d = 0.85
    n = X_norm.shape[0]
    E = np.ones(X_norm.shape) / n
    X_damped = d * X_norm + (1 - d) * E
    
    # Find steady state vector using power iteration method
    steady_state_vector = np.ones(n) / n
    for _ in range(100):  # Perform 100 iterations
        steady_state_vector = X_damped @ steady_state_vector
    
    return steady_state_vector

matrix2 = [
    [0, 0, 1, 0, 0, 0, 0, 0],
    [1, 0, 0, 1, 0, 0, 0, 0],
    [1, 0, 0, 0, 0, 0, 0, 0],
    [1, 1, 1, 0, 0, 0, 0, 0],
    [0, 1, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 0, 1, 1],
    [0, 0, 0, 1, 1, 0, 0, 1],
    [0, 0, 0, 0, 0, 1, 0, 0]
]

print("Steady State Probabilities:")
print(pageRank(np.array(matrix2)))









import numpy as np
import matplotlib.pyplot as plt
import cv2






# Load the data
data = np.loadtxt('qbo.txt')

# If data is a 2D array, we need to select the appropriate column
if data.ndim == 2:
    data = data[:, 0]  # Assuming the first column contains the time series data

# Define the order of the model
p = 5  # Choose an appropriate value for p

# Construct embedding matrix
M = np.zeros((len(data) - p, p))
for i in range(len(data) - p):
    for j in range(p):
        M[i, j] = data[i + j]  # Corrected indexing operation

# Define target values
y = data[p:]

# Solve for coefficients using pseudo-inverse
coefficients = np.linalg.lstsq(M, y, rcond=None)[0]






# Predict values using the fitted model
predicted_values = M @ coefficients

# Calculate Mean Squared Error (MSE)
mse = np.mean((y - predicted_values)**2)

# Calculate R-squared
total_variation = np.sum((y - np.mean(y))**2)
r_squared = 1 - mse / total_variation

print("Mean Squared Error:", mse)
print("R-squared:", r_squared)






# Load the second time series (z)
z = np.loadtxt('qbo.txt')  # You may need to adjust the path if the file name is different

# If z is a 2D array, we need to select the appropriate column
if z.ndim == 2:
    z = z[:, 0]  # Assuming the first column contains the time series data

# Construct embedding matrix for cross-regression
M_cross = np.zeros((len(z) - p, p))
for i in range(p):
    M_cross[:, i] = z[i:len(z) - p + i]

# Define target values
y = data[p:]

# Solve for coefficients using pseudo-inverse
coefficients_cross = np.linalg.lstsq(M_cross, y, rcond=None)[0]






# Load grayscale image
image = cv2.imread('cameraman.jpg', cv2.IMREAD_GRAYSCALE)

# Display image
plt.imshow(image, cmap='gray')
plt.axis('off')
plt.show()






# Load the image
image = cv2.imread('cameraman.jpg', cv2.IMREAD_GRAYSCALE)

# Construct Laplacian matrix based on the dimensions of the original image
G = graphs.Grid2d(image.shape[0], image.shape[1])
L = G.L

# Flatten the image
image_flattened = image.flatten().reshape(-1, 1)  # Reshape to a column vector

# Check dimensions of L and image_flattened again
print("Dimensions of L:", L.shape)
print("Dimensions of image_flattened:", image_flattened.shape)
print("Number of pixels in the image:", image_flattened.shape[0])

# Ensure that the Laplacian matrix has compatible dimensions with the flattened image vector
assert L.shape[0] == L.shape[1] == image_flattened.shape[0], "Dimension mismatch between L and image_flattened"



from scipy.sparse.linalg import lsqr

# Flatten the image and reshape to a column vector
image_flattened = image.flatten().reshape(-1, 1)

# Ensure that the Laplacian matrix L has compatible dimensions
assert L.shape[1] == image_flattened.shape[0], "Dimension mismatch between L and image_flattened"

# Apply the filter using lsqr
filtered_image_flattened = lsqr(L, image_flattened)[0]

# Reshape filtered image
filtered_image = filtered_image_flattened.reshape(image.shape)

# Display filtered image
plt.imshow(filtered_image, cmap='gray')
plt.axis('off')
plt.show()



get_ipython().getoutput("pip install pygsp")

from pygsp import graphs, filters, plotting


### do not edit this cell 
G = graphs.Grid2d(128,128)
L = G.L





print("Dimensions of L_reshaped:", L_reshaped.shape)
print("Dimensions of filtered_image_flattened:", filtered_image_flattened.shape)


# Solve the inverse problem
reconstructed_image_flattened = lsqr(L, filtered_image_flattened)[0]

# Reshape reconstructed image
reconstructed_image = reconstructed_image_flattened.reshape(image.shape)

# Display original, filtered, and reconstructed images
plt.figure(figsize=(10, 5))

plt.subplot(1, 3, 1)
plt.imshow(image, cmap='gray')
plt.title('Original Image')
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(filtered_image, cmap='gray')
plt.title('Filtered Image')
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(reconstructed_image, cmap='gray')
plt.title('Reconstructed Image')
plt.axis('off')

plt.show()          






### START CODE HERE ###


### END CODE HERE ###















